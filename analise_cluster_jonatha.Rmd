---
title: "Análise de Cluster"
author: "Jonatha Azevedo"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Nesse relatório, encontra-se duas abordagens para algoritmos de agrupamentos, um hierárquico e um não hierárquico. Com a proposta de usar um banco de dados e simular uma clusterização, iremos definir os clusters com base na metodologia definida em sala de aula. E usaremos alguma métrica pra definir se o agrupamento foi feito de uma forma razoável.


### Análise Exploratória dos dados
```{r dados}
#Carreganod os dados
dados <- read.csv2("Dados_Vendas.csv", header=T, stringsAsFactors = F)
head(dados)
```


Antes de entrar na modelagem de clusters, precisamos conhecer os dados. Temos, para esse trabalho, dados de vendas de vendedores de uma certa loja.

```{r exploratoria1}
summary(dados)
```

```{r exporatoria2, message=FALSE,fig.align='center'}
require(corrplot)
names(dados) <- names(dados)
corrplot(cor(dados))
  
```

Podemos ver que temos bastantes correlação entre as variáveis. Fato que pode auxiliar na clusterização. Embora, nesse trabalho o processo seja geral e não levando em conta alguma variável para definir melhor os centróides de cada cluster. 

## Análise de cluster


Conhecido como "agrupamento", é um conjunto de técnicas que permeite os agrupar elementos de um banco de dados de acordo com suas características, facilitando reconhecer relações e padrões existentes entre os indivíduos de cada grupo ***(*cluster*)**.

Sempre queremos formar cluster que possuem grande homogeniedade dentro de si e grande enterogeneidade entre os cluster, entre os grupos. Então, intuitivamente, queremos obter medidas de similaridades para organizar e distribuir os elementos no banco de dados aos clusters.

Por definições encontradas em artigos, temos as duas abordagens citadas a cima: 

  - **Hierárquica**: se divide nas abordagens aglomerativas ou divisivas;
  - **Não hierárquica**: baseada no famoso algoritmo de K-means.

A algumas medidas de similaridades, mas largamente mais usada é a de distância. Por representar melhor o conceito de proximidade, entre outras razões. A principal ferramenta nesse caso, é a **matriz de Similaridade (SM)**.

### Método hierárquico


```{r k}
k = 6
```

Não necessáriamente, precisamos sempre definir o número de clusters para a modelagem. O ideal seria obter esse número de uma forma variante e específica para cada contexto. 

Usando o método euclidiano (o **`R`** com a função **`dist()`**, possui 6 métodos: euclidiano, por máximo, manhattam, camberra, binário e minkowski) para calcular a distância, ou seja: $d(X_{i},X_{j}) = \sqrt{\sum\limits^{p}_{h=1}(X_{hi} - X_{hj})}$, logo a matriz de similaridade, conterá todos os valores $d(X_{i},X_{j})$ o que facilita a identificação dos pares mais similares.


```{r matrix de distancia}
dist <- dist(dados,method = "euclidean" ,diag=T, upper=T)  
```

```{r, message = FALSE,fig.align='center'}
clusters <- hclust(dist, "single")
plot(clusters)
```

O dendograma anteior não está muito claro para definirmos um número adequado ou razoavel de clusters. Logo, vamos utilizar alguns parâmetros na função **`hclust`**. 

```{r, message = FALSE,fig.align='center'}

dend1.1=hclust(dist(dados, method = "euclidean"),"single") # method - o metodo utilizado (completa, simples, ward...)
plot(dend1.1)
rect.hclust(dend1.1, k = 6, border = "blue")

dend1.2=hclust(dist,"complete") # method - o metodo utilizado (completa, simples, ward...)
plot(dend1.2)
rect.hclust(dend1.2, k = 6, border = "green")

dend1.3=hclust(dist,"ward.D2") # method - o metodo utilizado (completa, simples, ward...)
plot(dend1.3)
rect.hclust(dend1.3, k = 6, border = "red")

```

Precisamos de uma métrica para definir se a clusterização foi razoavel. Talvez olhar para a distribuição de elementos dentro de cada clusters seja uma medida a se considerar. Ou seja, quanto mais equilibrado o número de lementos em cada cluster, quer dizer que a clusterização foi adequada. 


Logo o **`hclust`** com método **`"ward.D2"`** (baseado no algoritmo de Ward, o Ward.D2 implementa o critério de elevação ao quadrado das distâncias euclidianas entres os pares de elementos em relação ao centróide), pela métrica citada, parece ser o melhor.



Primeiro adiciono o indice que refenrica o elemento a seu cluster.

```{r}

dados_clusters <- cbind(dados,grupo=cutree(dend1.3,h=6))
head(dados_clusters)
```

Repare que nesse novo conjunto de dados eu tenho uma variável chamada **`grupo`** ela indica a que grupo (de 1 a 6) cada vendedor percente. Com os grupos e seus elementos definidos, podemos trabalhar e aplicar comparações. 

Abaixo, a ideia é chegar ao mesmo resultado, só que considerando um método não hierárquico. Utilizando o algoritmo de **K-means**. 


### Método não hierárquico

Utilizando o algoritmo já implementado no R, faremos os clusters definindo inicialmente os números da quantidade de centros. 


O algorítmo de k-means é uma iteração, basicamente, ele consiste em fazer uma escolha inicial dos k elementos que formam as sementes iniciais. Esta escolha pode ser feita da seguinte forma:

  - Selecionado as k primeiras observações
  - Selecionando k observações aleatorioamente; e
  - Escolhendo k observações de modo que seus valores sejam bastente diferentes.


Escolhida as sementes iniciais, é calculada a distância de cada elemento em relação às sementes., agrupando o elemento ao grupo que possuir a menor distância e recalculando o centróide do mesmo. O procedimento, naturalmente, é repetido até que todos os elementos facam parte de um dos clusters.


Com isso  permitimos que os centróides iniciados aleatoriamente
se ajustem e melhorem a similaridade intragrupo. Isto é feito minimizando o erro do
agrupamento, que é definida como a soma das distâncias euclidianas entre cada indivíduo
e o centróide de seu respectivo grupo, isto é,

$$E = \sum\limits_{K}^{k=1}\sum\limits_{X_{i}\in C_{k}}^{}d(X_{i},y_{k})$$  


onde $K$ é o total de grupos definidos, $d(X_{i}, y_{k})$ é a distância entre os indivíduos $X_{i}$ e o centróide $y_{k}$ do cluster $C_{k}$. O método de cálculo da distância também é o euclidiano definido anteriormente.

Faremos para 3 casos: 

```{r k means}
kmeans_out1 <- kmeans(dados,centers = 4)
kmeans_out2 <- kmeans(dados,centers = 5)
kmeans_out3 <- kmeans(dados,centers = 6)
sizes <- list(size_4clusters = kmeans_out1$size,size_5clusters = kmeans_out2$size,size_6clusters = kmeans_out3$size);sizes
dados_clusters_kmeans = cbind(dados,grupo = kmeans_out3$cluster)
head(dados_clusters_kmeans)
```



Veja abaixo os centroids definidos: 

```{r,message = FALSE,fig.align="center"}
plot(dados, col = c(1,3,5,4,6,7))
points(kmeans_out3$centers, col = 1:4, pch = 8)
sort(kmeans_out3$cluster)
```




Como são poucos dados, optar por um número de clusters muito alto não parece nos dar muito ganho. Usamos uma métrica para medir o quão razoável foi a clusterização verificando o balanceamento da distribuição de elementos em cada grupo. Então por isso, vamos escolher 6 centros, como anteiormente. 





